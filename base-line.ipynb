{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7347d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCASDataset(Dataset):\n",
    "    def __init__(self, imgs, labels):\n",
    "        super().__init__()\n",
    "        self.imgs = torch.tensor(imgs, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagecas_dataset = ImageCASDataset(img_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_data = np.zeros(shape=(14, 512, 512, 275))\n",
    "img_data = []\n",
    "label_data = []\n",
    "\n",
    "for i in range(1, 15):\n",
    "    img = nib.load(f'./archive/1-200/{i}.img.nii.gz')\n",
    "    print(img.shape)\n",
    "    label = nib.load(f'./archive/1-200/{i}.label.nii.gz')\n",
    "\n",
    "    # img_data[i] = img.get_fdata()[:, :, :275]\n",
    "    img_data.append(img.get_fdata())\n",
    "    label_data.append(label.get_fdata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    \"\"\"(Conv3D -> BN -> ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv3D(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv3D(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Padding если размеры не совпадают\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        \n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                                     diffY // 2, diffY - diffY // 2,\n",
    "                                     diffZ // 2, diffZ - diffZ // 2])\n",
    "        \n",
    "        # Concatenate\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=32):\n",
    "        super(UNet3D, self).__init__()\n",
    "        \n",
    "        features = init_features\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = DoubleConv3D(in_channels, features)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.encoder2 = DoubleConv3D(features, features * 2)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.encoder3 = DoubleConv3D(features * 2, features * 4)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.encoder4 = DoubleConv3D(features * 4, features * 8)\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv3D(features * 8, features * 16)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose3d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv3D(features * 16, features * 8)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose3d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv3D(features * 8, features * 4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose3d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv3D(features * 4, features * 2)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose3d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConv3D(features * 2, features)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv3d(features, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        \n",
    "        # Decoder\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        \n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        return self.out(dec1)\n",
    "\n",
    "\n",
    "# Тест модели\n",
    "if __name__ == '__main__':\n",
    "    model = UNet3D(in_channels=1, out_channels=1, init_features=32)\n",
    "    x = torch.randn(1, 1, 64, 64, 64)  # Batch, Channels, D, H, W\n",
    "    output = model(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Flatten\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (predictions * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (\n",
    "            predictions.sum() + targets.sum() + self.smooth\n",
    "        )\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.dice = DiceLoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice_weight = dice_weight\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        return (self.dice_weight * self.dice(predictions, targets) + \n",
    "                (1 - self.dice_weight) * self.bce(predictions, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0082c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=50, lr=1e-4):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = CombinedLoss(dice_weight=0.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}')\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Сохранение лучшей модели\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_unet3d.pth')\n",
    "            print('✓ Model saved!')\n",
    "\n",
    "# Запуск\n",
    "model = UNet3D(in_channels=1, out_channels=1, init_features=32)\n",
    "train_model(model, train_loader, val_loader, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
